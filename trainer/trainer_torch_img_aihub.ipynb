{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "# import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist_1= os.listdir('data/')\n",
    "type(mylist_1)\n",
    "mylist=[]\n",
    "for item in mylist_1:\n",
    "    if item[-3:]=='wav':\n",
    "        mylist.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav2spec\n",
    "class ETRIDataset_spectram():\n",
    "    def __init__(self, file_list, frame_length=0.025, frame_stride=0.010):\n",
    "        self.file_list = file_list\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_stride = frame_stride\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        audio_path = self.file_list[index]\n",
    "        X, sample_rate = librosa.load('data/'+audio_path, res_type='kaiser_fast',duration=2.5,sr=16000,offset=0.0)\n",
    "        signal = np.zeros((int(sample_rate *3,)))\n",
    "        signal[:len(X)] = X\n",
    "        sample_rate = sample_rate\n",
    "        input_nfft = int(round(sample_rate*self.frame_length))\n",
    "        input_stride = int(round(sample_rate*self.frame_stride))\n",
    "\n",
    "        S = librosa.feature.melspectrogram(y=X, n_mels=64, n_fft=input_nfft, hop_length=input_stride)\n",
    "        P = librosa.power_to_db(S, ref=np.max)\n",
    "        \n",
    "\n",
    "        ## get label\n",
    "        if audio_path[-3:] == 'wav':\n",
    "            if audio_path[7:8] =='a':\n",
    "                label = 0\n",
    "            elif audio_path[7:8] =='n':\n",
    "                label = 1\n",
    "            elif audio_path[7:8] =='s':\n",
    "                label = 2\n",
    "            elif audio_path[7:8] =='h':\n",
    "                label = 3\n",
    "        else:\n",
    "            label=None\n",
    "        return P, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "576\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# train,test,val split\n",
    "train_size = int(0.7*len(mylist))\n",
    "\n",
    "val_size = int(0.2*len(mylist))\n",
    "test_size = int(len(mylist)-train_size-val_size)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(ETRIDataset_spectram(mylist),[train_size,val_size,test_size])\n",
    "print(train_size)\n",
    "print(val_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec2img\n",
    "def getimg(dataset,feature):\n",
    "        img_path=[]\n",
    "        labels=[]\n",
    "        for i in range(len(dataset)):\n",
    "                fig = plt.figure()\n",
    "                ax = fig.add_subplot(111)\n",
    "                p = librosa.display.specshow(dataset[i][0],ax=ax, sr=16000, hop_length=int(round(16000*0.025)), x_axis='time',y_axis='linear')\n",
    "                extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "\n",
    "                fig.savefig('./image/%s_%d_%s.jpg' % (dataset[i][1],i,feature), bbox_inches=extent)\n",
    "                img_path.append('./image/%s_%d_%s.jpg' % (dataset[i][1],i,feature))\n",
    "                labels.append(dataset[i][1])\n",
    "                plt.ioff()\n",
    "                plt.close()\n",
    "        return img_path , labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path , train_labels = getimg(train_set,'train')\n",
    "val_path , val_labels = getimg(val_set,'val')\n",
    "test_path , test_labels = getimg(test_set,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img2tensor():\n",
    "    def __init__(self,data_path,labels,transforms):\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data_path[index]\n",
    "        image = Image.open(img_path)\n",
    "        I = train_transforms(image)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return I, label\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "tensor([1, 0, 3, 1, 1, 2, 3, 2, 2, 2, 2, 3, 3, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# set batch_size\n",
    "batch_size = 16\n",
    "# dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(img2tensor(train_path,train_labels,train_transforms), batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(img2tensor(val_path,val_labels,test_transforms), batch_size=batch_size, shuffle=False)\n",
    "dataloaders_dict ={'train':train_dataloader, 'val': val_dataloader}\n",
    "# test\n",
    "batch_iterator = iter(dataloaders_dict['train'])\n",
    "inputs, labels = next(batch_iterator)\n",
    "print(inputs.size())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  (classifier): Linear(in_features=1000, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "model.to(torch.device('cuda'))\n",
    "model.classifier = nn.Linear(in_features=1000, out_features=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters() ,lr=0.00001, weight_decay=1e-6, momentum=0.9)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(net, dataloader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print('-------------------------------')\n",
    "\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "\n",
    "            if (epoch == 0) and(phase == 'train'):\n",
    "                continue\n",
    "            for inputs, labels in tqdm(dataloader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs,1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() *inputs.size(0)\n",
    "\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    epoch_loss = epoch_loss / len(dataloader[phase].dataset)\n",
    "                    epoch_acc = epoch_corrects.double() / len(dataloader[phase].dataset)\n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f} ACC {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7948/856878266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders_dict' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, dataloaders_dict, criterion,optimizer, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weight\n",
    "save_path = './saved_models/DenseNet121_img.pth'\n",
    "torch.save(model.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weight\n",
    "load_path = './saved_models/DenseNet121_img.pth'\n",
    "load_weights = torch.load(load_path, map_location={'cuda:0': 'cpu'})\n",
    "model = torchvision.models.densenet121(pretrained=True)\n",
    "# first_conv_layer = [nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)]\n",
    "# first_conv_layer.extend(list(model.features))  \n",
    "# model.features= nn.Sequential(*first_conv_layer )  \n",
    "model.classifier = nn.Linear(in_features=1024, out_features=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters() ,lr=0.0001, weight_decay=1e-6, momentum=0.9)\n",
    "model.eval()\n",
    "model.load_state_dict(load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self, model, device ='cpu',  fp16=False ):\n",
    "        self.model = model\n",
    "        \n",
    "        self.cls_name = {0:'angry', 1:'happy', 2:'sad', 3:'neutral'}\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def predict(self, audio):\n",
    "        \n",
    "        audio_info = audio\n",
    "        \n",
    "        outputs = self.model(audio_info)\n",
    "        probability = torch.softmax(outputs,1)\n",
    "        probability = probability.squeeze()\n",
    "        proba, idx = torch.max(probability, dim=0)\n",
    "        emo_proba = proba.item()\n",
    "        print(emo_proba)\n",
    "        idx = idx.item()\n",
    "        emo_label = self.cls_name[idx]\n",
    "        print(emo_label)\n",
    "        return emo_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swdata(emotion):\n",
    "    swlist_1 = os.listdir('swdata/%s/' % emotion)\n",
    "    type(swlist_1)\n",
    "    swlist = []\n",
    "    for item in swlist_1:\n",
    "        if item[-3:] == 'wav':\n",
    "            swlist.append('./swdata/%s/' % emotion + item)\n",
    "    return swlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = swdata('angry') + swdata('happy') + swdata('sad') + swdata('neutral')\n",
    "\n",
    "random.shuffle(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav2spec\n",
    "class swdata_spectram():\n",
    "    def __init__(self, file_list, frame_length=0.025, frame_stride=0.010):\n",
    "        self.file_list = file_list\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_stride = frame_stride\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        audio_path = self.file_list[index]\n",
    "        X, sample_rate = librosa.load(audio_path, res_type='kaiser_fast',duration=2.5,sr=16000,offset=0.0)\n",
    "        signal = np.zeros((int(sample_rate *3,)))\n",
    "        signal[:len(X)] = X\n",
    "        sample_rate = sample_rate\n",
    "        input_nfft = int(round(sample_rate*self.frame_length))\n",
    "        input_stride = int(round(sample_rate*self.frame_stride))\n",
    "\n",
    "        S = librosa.feature.melspectrogram(y=X, n_mels=64, n_fft=input_nfft, hop_length=input_stride)\n",
    "        P = librosa.power_to_db(S, ref=np.max)\n",
    "        \n",
    "\n",
    "        ## get label\n",
    "        if audio_path[-3:] == 'wav':\n",
    "            if audio_path[9:10] =='a':\n",
    "                label = 0\n",
    "            elif audio_path[9:10] =='n':\n",
    "                label = 1\n",
    "            elif audio_path[9:10] =='s':\n",
    "                label = 2\n",
    "            elif audio_path[9:10] =='h':\n",
    "                label = 3\n",
    "        else:\n",
    "            label=None\n",
    "        return P, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_swdata = swdata_spectram(sw_list)\n",
    "test_path , test_labels = getimg(test_swdata,'test_sw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./image/0_0_test_sw.jpg',\n",
       " './image/1_1_test_sw.jpg',\n",
       " './image/0_2_test_sw.jpg',\n",
       " './image/3_3_test_sw.jpg',\n",
       " './image/0_4_test_sw.jpg',\n",
       " './image/2_5_test_sw.jpg',\n",
       " './image/0_6_test_sw.jpg',\n",
       " './image/1_7_test_sw.jpg',\n",
       " './image/0_8_test_sw.jpg',\n",
       " './image/1_9_test_sw.jpg',\n",
       " './image/1_10_test_sw.jpg',\n",
       " './image/0_11_test_sw.jpg',\n",
       " './image/0_12_test_sw.jpg',\n",
       " './image/1_13_test_sw.jpg',\n",
       " './image/0_14_test_sw.jpg',\n",
       " './image/0_15_test_sw.jpg',\n",
       " './image/2_16_test_sw.jpg',\n",
       " './image/2_17_test_sw.jpg',\n",
       " './image/1_18_test_sw.jpg',\n",
       " './image/3_19_test_sw.jpg',\n",
       " './image/2_20_test_sw.jpg',\n",
       " './image/0_21_test_sw.jpg',\n",
       " './image/1_22_test_sw.jpg',\n",
       " './image/1_23_test_sw.jpg',\n",
       " './image/2_24_test_sw.jpg',\n",
       " './image/1_25_test_sw.jpg',\n",
       " './image/1_26_test_sw.jpg',\n",
       " './image/2_27_test_sw.jpg',\n",
       " './image/1_28_test_sw.jpg',\n",
       " './image/2_29_test_sw.jpg',\n",
       " './image/2_30_test_sw.jpg',\n",
       " './image/1_31_test_sw.jpg',\n",
       " './image/0_32_test_sw.jpg',\n",
       " './image/1_33_test_sw.jpg',\n",
       " './image/2_34_test_sw.jpg',\n",
       " './image/2_35_test_sw.jpg',\n",
       " './image/1_36_test_sw.jpg',\n",
       " './image/1_37_test_sw.jpg',\n",
       " './image/3_38_test_sw.jpg',\n",
       " './image/1_39_test_sw.jpg',\n",
       " './image/1_40_test_sw.jpg',\n",
       " './image/3_41_test_sw.jpg',\n",
       " './image/2_42_test_sw.jpg',\n",
       " './image/3_43_test_sw.jpg',\n",
       " './image/3_44_test_sw.jpg',\n",
       " './image/1_45_test_sw.jpg',\n",
       " './image/3_46_test_sw.jpg',\n",
       " './image/3_47_test_sw.jpg',\n",
       " './image/0_48_test_sw.jpg',\n",
       " './image/0_49_test_sw.jpg',\n",
       " './image/0_50_test_sw.jpg',\n",
       " './image/0_51_test_sw.jpg',\n",
       " './image/0_52_test_sw.jpg',\n",
       " './image/2_53_test_sw.jpg',\n",
       " './image/3_54_test_sw.jpg',\n",
       " './image/3_55_test_sw.jpg',\n",
       " './image/1_56_test_sw.jpg',\n",
       " './image/0_57_test_sw.jpg',\n",
       " './image/2_58_test_sw.jpg',\n",
       " './image/1_59_test_sw.jpg',\n",
       " './image/3_60_test_sw.jpg',\n",
       " './image/2_61_test_sw.jpg',\n",
       " './image/3_62_test_sw.jpg',\n",
       " './image/3_63_test_sw.jpg',\n",
       " './image/2_64_test_sw.jpg',\n",
       " './image/3_65_test_sw.jpg',\n",
       " './image/3_66_test_sw.jpg',\n",
       " './image/1_67_test_sw.jpg',\n",
       " './image/2_68_test_sw.jpg',\n",
       " './image/0_69_test_sw.jpg',\n",
       " './image/2_70_test_sw.jpg',\n",
       " './image/3_71_test_sw.jpg',\n",
       " './image/1_72_test_sw.jpg',\n",
       " './image/3_73_test_sw.jpg',\n",
       " './image/2_74_test_sw.jpg',\n",
       " './image/2_75_test_sw.jpg',\n",
       " './image/2_76_test_sw.jpg',\n",
       " './image/3_77_test_sw.jpg',\n",
       " './image/0_78_test_sw.jpg',\n",
       " './image/0_79_test_sw.jpg',\n",
       " './image/3_80_test_sw.jpg',\n",
       " './image/3_81_test_sw.jpg',\n",
       " './image/0_82_test_sw.jpg',\n",
       " './image/2_83_test_sw.jpg',\n",
       " './image/3_84_test_sw.jpg',\n",
       " './image/2_85_test_sw.jpg',\n",
       " './image/3_86_test_sw.jpg',\n",
       " './image/0_87_test_sw.jpg',\n",
       " './image/0_88_test_sw.jpg']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2tensor(test_path,test_labels,test_transforms)[1][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97287917137146\n",
      "angry\n",
      "0.47407740354537964\n",
      "sad\n",
      "0.6134857535362244\n",
      "angry\n",
      "0.9966476559638977\n",
      "sad\n",
      "0.8344489932060242\n",
      "angry\n",
      "0.98235023021698\n",
      "sad\n",
      "0.9849558472633362\n",
      "angry\n",
      "0.9999333620071411\n",
      "angry\n",
      "0.6899652481079102\n",
      "happy\n",
      "0.9631986618041992\n",
      "angry\n",
      "0.7478828430175781\n",
      "happy\n",
      "0.5698075890541077\n",
      "neutral\n",
      "0.7549975514411926\n",
      "sad\n",
      "0.622043251991272\n",
      "angry\n",
      "0.9752891063690186\n",
      "sad\n",
      "0.574131965637207\n",
      "angry\n",
      "0.4281877279281616\n",
      "angry\n",
      "0.9951044321060181\n",
      "sad\n",
      "0.5460681915283203\n",
      "neutral\n",
      "0.9759037494659424\n",
      "angry\n",
      "0.9977781176567078\n",
      "sad\n",
      "0.8374400734901428\n",
      "happy\n",
      "0.6592076420783997\n",
      "happy\n",
      "0.46127983927726746\n",
      "sad\n",
      "0.7378534078598022\n",
      "angry\n",
      "0.5901894569396973\n",
      "happy\n",
      "0.4655432105064392\n",
      "happy\n",
      "0.8803954124450684\n",
      "angry\n",
      "0.6913800835609436\n",
      "angry\n",
      "0.997877836227417\n",
      "sad\n",
      "0.8824412226676941\n",
      "sad\n",
      "0.6370543241500854\n",
      "neutral\n",
      "0.988271176815033\n",
      "angry\n",
      "0.9982762336730957\n",
      "angry\n",
      "0.9966848492622375\n",
      "sad\n",
      "0.9987173080444336\n",
      "sad\n",
      "0.9990296363830566\n",
      "angry\n",
      "0.5206223726272583\n",
      "happy\n",
      "0.5789490938186646\n",
      "angry\n",
      "0.955120861530304\n",
      "angry\n",
      "0.5529403686523438\n",
      "angry\n",
      "0.7743945717811584\n",
      "sad\n",
      "0.6016064882278442\n",
      "sad\n",
      "0.5679851770401001\n",
      "angry\n",
      "0.9996684789657593\n",
      "sad\n",
      "0.5204717516899109\n",
      "sad\n",
      "0.9954782128334045\n",
      "angry\n",
      "0.9994773268699646\n",
      "sad\n",
      "0.5031684637069702\n",
      "sad\n",
      "0.8461170792579651\n",
      "neutral\n",
      "0.891269326210022\n",
      "sad\n",
      "0.9339776635169983\n",
      "happy\n",
      "0.9119904637336731\n",
      "happy\n",
      "0.9906033873558044\n",
      "sad\n",
      "0.946254312992096\n",
      "neutral\n",
      "0.911295473575592\n",
      "sad\n",
      "0.9881608486175537\n",
      "sad\n",
      "0.8466699719429016\n",
      "sad\n",
      "0.6828997731208801\n",
      "happy\n",
      "0.996168315410614\n",
      "angry\n",
      "0.7750847935676575\n",
      "sad\n",
      "0.7439412474632263\n",
      "sad\n",
      "0.9239555597305298\n",
      "angry\n",
      "0.9845667481422424\n",
      "angry\n",
      "0.8962957262992859\n",
      "angry\n",
      "0.9882072806358337\n",
      "sad\n",
      "0.6724604964256287\n",
      "angry\n",
      "0.9701284766197205\n",
      "sad\n",
      "0.48673170804977417\n",
      "happy\n",
      "0.9993539452552795\n",
      "sad\n",
      "0.7639085650444031\n",
      "angry\n",
      "0.996887743473053\n",
      "sad\n",
      "0.9961124062538147\n",
      "angry\n",
      "0.9042495489120483\n",
      "sad\n",
      "0.9413396716117859\n",
      "sad\n",
      "0.4878813326358795\n",
      "angry\n",
      "0.9436360001564026\n",
      "angry\n",
      "0.7277660369873047\n",
      "neutral\n",
      "0.49634018540382385\n",
      "happy\n",
      "0.554646372795105\n",
      "sad\n",
      "0.7514582872390747\n",
      "angry\n",
      "0.9781414866447449\n",
      "sad\n",
      "0.9283888936042786\n",
      "sad\n",
      "0.6902788877487183\n",
      "sad\n",
      "0.9874862432479858\n",
      "angry\n",
      "0.8983322978019714\n",
      "sad\n",
      "0.8733922243118286\n",
      "sad\n",
      "0.8466579914093018\n",
      "sad\n",
      "0.8586305975914001\n",
      "sad\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model)\n",
    "device='cpu'\n",
    "a=[]\n",
    "b=[]\n",
    "for i in range(len(img2tensor(test_path,test_labels,test_transforms))):\n",
    "    a.append(predictor.predict(img2tensor(test_path,test_labels,test_transforms)[i][0].unsqueeze(0)))\n",
    "    b.append(img2tensor(test_path,test_labels,test_transforms)[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.29213483146067415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21177/4207275683.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['original'][i] = 'angry'\n",
      "/home/seojungin/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(a,columns=['predict'])\n",
    "df['original']=b\n",
    "for i in range(len(df)):\n",
    "    if df['original'][i] == 0:\n",
    "        df['original'][i] = 'angry'\n",
    "    elif df['original'][i] == 1:\n",
    "        df['original'][i] = 'happy'\n",
    "    elif df['original'][i] == 2:\n",
    "        df['original'][i] = 'sad'\n",
    "    elif df['original'][i] == 3:\n",
    "        df['original'][i] = 'neutral'\n",
    "print('accuracy={}'.format((df['predict']==df['original']).sum()/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('predict_result_img_sw.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sad</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sad</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>angry</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>sad</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>sad</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>sad</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   predict original\n",
       "0    angry    angry\n",
       "1      sad    happy\n",
       "2    angry    angry\n",
       "3      sad  neutral\n",
       "4    angry    angry\n",
       "..     ...      ...\n",
       "84   angry  neutral\n",
       "85     sad      sad\n",
       "86     sad  neutral\n",
       "87     sad    angry\n",
       "88     sad    angry\n",
       "\n",
       "[89 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cd0a2b2755993ea388295ce13640730b09dff1872912d88e6835a7e20d63119"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
